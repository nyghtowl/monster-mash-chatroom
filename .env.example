# Monster Mash Chatroom Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Demo Mode (default: true)
# =============================================================================
# When true, monsters respond with personality-driven scripted responses
# When false, monsters call real LLMs (requires API keys below)
DEMO_MODE=true

# =============================================================================
# Message Bus Configuration
# =============================================================================
# Backend: "in-memory" or "kafka"
# - in-memory: Simple, single-server, no external dependencies (default)
# - kafka: Distributed, supports multiple servers and workers
# BUS__BACKEND=kafka

# Kafka Broker Configuration (only used when BUS__BACKEND=kafka)
# Option 1: Numbered brokers (recommended for multiple brokers)
# BUS__KAFKA__BROKERS__0=localhost:29092
# BUS__KAFKA__BROKERS__1=localhost:29093

# Option 2: Comma-separated brokers (alternative syntax)
# BUS__KAFKA__BROKERS=localhost:29092,localhost:29093

# Kafka topic name for chat messages
# BUS__KAFKA__TOPIC=monster.chat

# Consumer group namespace prefix for persona workers
BUS__NAMESPACE=monster-mash-chatroom

# Number of messages kept in memory for new WebSocket clients
BUS__HISTORY_LIMIT=200

# =============================================================================
# LLM Configuration (only used when DEMO_MODE=false)
# =============================================================================
# Default model for all personas (unless overridden by persona map)
MODEL_ROUTING__DEFAULT_MODEL=gpt-4o-mini

# Per-persona model routing (optional)
# JSON format: {"persona_key": "model_name"}
# Example: MODEL_ROUTING__PERSONA_MODEL_MAP={"witch":"gpt-4","vampire":"claude-3-5-sonnet-20241022"}
MODEL_ROUTING__PERSONA_MODEL_MAP=

# =============================================================================
# LLM Provider API Keys (uncomment the one you're using)
# =============================================================================

# OpenAI (GPT-4, GPT-4o, GPT-3.5-turbo, etc.)
# OPENAI_API_KEY=sk-...

# Anthropic (Claude 3.5 Sonnet, Claude 3 Opus, etc.)
# ANTHROPIC_API_KEY=sk-ant-...

# Azure OpenAI
# AZURE_API_KEY=...
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-01

# Google AI (Gemini)
# GEMINI_API_KEY=...

# Cohere
# COHERE_API_KEY=...

# Hugging Face
# HUGGINGFACE_API_KEY=...

# =============================================================================
# Ollama (Local LLM)
# =============================================================================
# Ollama runs models locally without API keys
# Install: https://ollama.ai
# Models: llama3.2, mistral, phi3, etc.
# 
# To use Ollama:
# 1. Install Ollama and start it
# 2. Pull a model: ollama pull llama3.2
# 3. Set DEMO_MODE=false
# 4. Set DEFAULT_MODEL=ollama/llama3.2
#
# Example configuration:
# DEMO_MODE=false
# MODEL_ROUTING__DEFAULT_MODEL=ollama/llama3.2
# OLLAMA_API_BASE=http://localhost:11434
#
# Per-persona routing with Ollama:
# MODEL_ROUTING__PERSONA_MODEL_MAP={"witch":"ollama/llama3.2","vampire":"ollama/mistral","ghost":"ollama/phi3"}

# =============================================================================
# Server Configuration
# =============================================================================
# Port for the FastAPI server (only affects run.sh script)
# UVICORN_PORT=8000

# Log level (DEBUG, INFO, WARNING, ERROR)
# UVICORN_LOG_LEVEL=INFO

